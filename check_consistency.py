#!/usr/bin/env python3
"""
ë¬´ì´ëª° ì›¹ì‚¬ì´íŠ¸ ì¼ê´€ì„± ì²´í¬ ìŠ¤í¬ë¦½íŠ¸
Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  í˜ì´ì§€ë¥¼ ë¶„ì„í•˜ê³  ë¶ˆì¼ì¹˜ ì‚¬í•­ì„ ì°¾ìŠµë‹ˆë‹¤.
"""

from playwright.sync_api import sync_playwright
import json
import re
from datetime import datetime

def analyze_page(page, url, page_name):
    """í˜ì´ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ì£¼ìš” ì •ë³´ ì¶”ì¶œ"""
    print(f"\n{'='*80}")
    print(f"ğŸ“„ ë¶„ì„ ì¤‘: {page_name}")
    print(f"{'='*80}")

    page.goto(url)
    page.wait_for_load_state('networkidle')

    # í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    body_text = page.inner_text('body')

    # ë¶„ì„ ê²°ê³¼ ì €ì¥
    analysis = {
        'page_name': page_name,
        'url': url,
        'issues': [],
        'info': {}
    }

    # 1. ë¸Œëœë“œëª… ì²´í¬
    print("\nğŸ” ë¸Œëœë“œëª… ë¶„ì„:")
    brand_mentions = {
        'ë¬´ì´ìƒí’ˆê¶Œ': body_text.count('ë¬´ì´ìƒí’ˆê¶Œ'),
        'ë¬´ì´ëª°': body_text.count('ë¬´ì´ëª°'),
        'MU2MALL': body_text.count('MU2MALL'),
        'ë¬´ì´ìƒë¸”ë¡': body_text.count('ë¬´ì´ìƒë¸”ë¡')
    }

    for brand, count in brand_mentions.items():
        if count > 0:
            print(f"  - '{brand}': {count}íšŒ ë°œê²¬")
            analysis['info'][f'brand_{brand}'] = count

    # ë¬´ì´ìƒí’ˆê¶Œì´ ë‚¨ì•„ìˆëŠ”ì§€ ì²´í¬ (ë¡œê³  alt ì œì™¸)
    if 'ë¬´ì´ìƒí’ˆê¶Œ' in body_text:
        # alt ì†ì„±ì´ ì•„ë‹Œ ì‹¤ì œ í…ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
        alt_count = len(re.findall(r'alt="ë¬´ì´ìƒí’ˆê¶Œ"', page.content()))
        text_count = brand_mentions['ë¬´ì´ìƒí’ˆê¶Œ']

        if text_count > alt_count:
            analysis['issues'].append({
                'type': 'brand_inconsistency',
                'severity': 'high',
                'description': f"'ë¬´ì´ìƒí’ˆê¶Œ'ì´ {text_count}ë²ˆ ë°œê²¬ë¨ (alt ì†ì„± {alt_count}ê°œ í¬í•¨). 'ë¬´ì´ëª°'ë¡œ í†µì¼ í•„ìš”"
            })

    # 2. ë‚ ì§œ ì²´í¬
    print("\nğŸ“… ë‚ ì§œ ë¶„ì„:")
    date_patterns = [
        r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼',
        r'(\d{4})-(\d{2})-(\d{2})',
        r'ê³µê³ ì¼ì:\s*(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼',
        r'ì‹œí–‰ì¼ì:\s*(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼'
    ]

    found_dates = []
    for pattern in date_patterns:
        matches = re.findall(pattern, body_text)
        for match in matches:
            date_str = '-'.join(match)
            found_dates.append(date_str)
            print(f"  - ë°œê²¬ëœ ë‚ ì§œ: {date_str}")

    analysis['info']['dates'] = found_dates

    # ë‚ ì§œ ì¼ê´€ì„± ì²´í¬
    unique_dates = set(found_dates)
    if len(unique_dates) > 3:  # ê³µê³ ì¼, ì‹œí–‰ì¼, ê°œì—…ì¼ ë“± í•©ë¦¬ì ì¸ ë²”ìœ„ ì´ˆê³¼
        analysis['issues'].append({
            'type': 'date_inconsistency',
            'severity': 'medium',
            'description': f"ì—¬ëŸ¬ ë‹¤ë¥¸ ë‚ ì§œ ë°œê²¬: {unique_dates}"
        })

    # 3. íšŒì‚¬ ì •ë³´ ì²´í¬
    print("\nğŸ¢ íšŒì‚¬ ì •ë³´ ë¶„ì„:")
    company_info = {
        'ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸': re.search(r'ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸[:\s]*([0-9\-]+)', body_text),
        'ë²•ì¸ë“±ë¡ë²ˆí˜¸': re.search(r'ë²•ì¸ë“±ë¡ë²ˆí˜¸[:\s]*([0-9\-]+)', body_text),
        'ëŒ€í‘œìëª…': re.search(r'ëŒ€í‘œì?[:\s]*([ê°€-í£]+)', body_text),
        'ì „í™”ë²ˆí˜¸': re.search(r'ì „í™”[:\s]*([\d\-]+)', body_text),
        'ì£¼ì†Œ': re.search(r'ì£¼ì†Œ[:\s]*([^\n]+)', body_text)
    }

    for key, match in company_info.items():
        if match:
            value = match.group(1).strip()
            print(f"  - {key}: {value}")
            analysis['info'][key] = value

    # 4. ì´ë©”ì¼ ì£¼ì†Œ ì²´í¬
    print("\nğŸ“§ ì´ë©”ì¼ ì£¼ì†Œ ë¶„ì„:")
    emails = re.findall(r'[\w\.-]+@[\w\.-]+\.\w+', body_text)
    unique_emails = set(emails)
    for email in unique_emails:
        print(f"  - {email}")
    analysis['info']['emails'] = list(unique_emails)

    # 5. ë§í¬ ì²´í¬
    print("\nğŸ”— ë§í¬ ë¶„ì„:")
    links = page.query_selector_all('a[href]')
    broken_links = []

    for link in links[:10]:  # ì²˜ìŒ 10ê°œë§Œ ì²´í¬ (ì‹œê°„ ì ˆì•½)
        href = link.get_attribute('href')
        if href and not href.startswith('#') and not href.startswith('javascript'):
            print(f"  - {href}")

            # ë¡œì»¬ íŒŒì¼ ë§í¬ ì²´í¬
            if href.endswith('.html'):
                if href.startswith('http'):
                    continue
                # ë¡œì»¬ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ì²´í¬
                try:
                    test_page = page.context.new_page()
                    test_page.goto(f"file:///Users/hasanghyeon/brother_site/muyi-giftcard/public/{href}", timeout=3000)
                    test_page.close()
                except Exception as e:
                    broken_links.append(href)
                    print(f"    âš ï¸ ë§í¬ ì˜¤ë¥˜: {str(e)[:50]}")

    if broken_links:
        analysis['issues'].append({
            'type': 'broken_links',
            'severity': 'medium',
            'description': f"ê¹¨ì§„ ë§í¬: {broken_links}"
        })

    # 6. ë¡œê³  ì´ë¯¸ì§€ ì²´í¬
    print("\nğŸ–¼ï¸ ë¡œê³  ì´ë¯¸ì§€ ë¶„ì„:")
    logo_imgs = page.query_selector_all('img[src*="logo"]')
    for img in logo_imgs:
        src = img.get_attribute('src')
        alt = img.get_attribute('alt')
        print(f"  - src: {src}, alt: {alt}")

    # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    screenshot_path = f"/Users/hasanghyeon/brother_site/muyi-giftcard/screenshots/{page_name}_{timestamp}.png"
    page.screenshot(path=screenshot_path, full_page=True)
    print(f"\nğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_path}")

    return analysis

def main():
    print("ğŸš€ ë¬´ì´ëª° ì›¹ì‚¬ì´íŠ¸ ì¼ê´€ì„± ì²´í¬ ì‹œì‘")
    print("="*80)

    pages_to_check = [
        {
            'url': 'file:///Users/hasanghyeon/brother_site/muyi-giftcard/public/index.html',
            'name': 'index'
        },
        {
            'url': 'file:///Users/hasanghyeon/brother_site/muyi-giftcard/public/terms.html',
            'name': 'terms'
        },
        {
            'url': 'file:///Users/hasanghyeon/brother_site/muyi-giftcard/public/privacy.html',
            'name': 'privacy'
        },
        {
            'url': 'file:///Users/hasanghyeon/brother_site/muyi-giftcard/public/refund.html',
            'name': 'refund'
        }
    ]

    all_results = []

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(viewport={'width': 1920, 'height': 1080})
        page = context.new_page()

        for page_info in pages_to_check:
            try:
                result = analyze_page(page, page_info['url'], page_info['name'])
                all_results.append(result)
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ ({page_info['name']}): {str(e)}")
                all_results.append({
                    'page_name': page_info['name'],
                    'error': str(e),
                    'issues': [{
                        'type': 'error',
                        'severity': 'critical',
                        'description': f"í˜ì´ì§€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
                    }]
                })

        browser.close()

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "="*80)
    print("ğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½")
    print("="*80)

    total_issues = 0
    critical_issues = 0
    high_issues = 0
    medium_issues = 0

    for result in all_results:
        page_name = result.get('page_name', 'Unknown')
        issues = result.get('issues', [])

        if issues:
            print(f"\nğŸ“„ {page_name}:")
            for issue in issues:
                total_issues += 1
                severity = issue.get('severity', 'unknown')

                if severity == 'critical':
                    critical_issues += 1
                    icon = 'ğŸ”´'
                elif severity == 'high':
                    high_issues += 1
                    icon = 'ğŸŸ '
                elif severity == 'medium':
                    medium_issues += 1
                    icon = 'ğŸŸ¡'
                else:
                    icon = 'âšª'

                print(f"  {icon} [{severity.upper()}] {issue.get('description', '')}")
        else:
            print(f"\nâœ… {page_name}: ë¬¸ì œ ì—†ìŒ")

    print("\n" + "="*80)
    print(f"ì´ ì´ìŠˆ: {total_issues}ê°œ")
    print(f"  ğŸ”´ Critical: {critical_issues}ê°œ")
    print(f"  ğŸŸ  High: {high_issues}ê°œ")
    print(f"  ğŸŸ¡ Medium: {medium_issues}ê°œ")
    print("="*80)

    # JSON íŒŒì¼ë¡œ ì €ì¥
    report_path = '/Users/hasanghyeon/brother_site/muyi-giftcard/consistency_report.json'
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“„ ìƒì„¸ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
    print("\nâœ… ë¶„ì„ ì™„ë£Œ!")

if __name__ == '__main__':
    main()
